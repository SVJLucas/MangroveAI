{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bFpoPl8MPwBo"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT5DNsXVQgLl",
    "outputId": "bd4c760a-e2dc-4f5c-cf10-362a5744f3a7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpg3fuxhcQBm"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEjSQEdcJwLJ"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install rasterio pytorch-lightning albumentations tensorboard\n",
    "!pip install rasterio torch pillow scikit-image pytorch-lightning segmentation-models-pytorch albumentations tensorboard\n",
    "!pip install causal-conv1d==1.1.1\n",
    "!pip install mamba-ssm\n",
    "!pip install nnunetv2 monai\n",
    "!pip install torchinfo numba timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1aD5xV-5cKMY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, jaccard_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "from skimage.io import imread, imshow\n",
    "\n",
    "from skimage.io import imread\n",
    "import albumentations as A\n",
    "from typing import List\n",
    "\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Optional, Union, Type, List, Tuple, Callable, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange, repeat\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n",
    "DropPath.__repr__ = lambda self: f\"timm.DropPath({self.drop_prob})\"\n",
    "\n",
    "from nnunetv2.utilities.plans_handling.plans_handler import ConfigurationManager, PlansManager\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock, UnetrUpBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXRXQqeH7COi"
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_all_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3NKqrA3Ig3y2"
   },
   "outputs": [],
   "source": [
    "# DATASET\n",
    "dataset_dir = \"dataset/\"\n",
    "# ADJUST THESE 2 VALUES DEPENDING ON THE TRAINING TIME\n",
    "train_test_split = 0.5\n",
    "n_samples_per_zone = 20 # 50 recommended\n",
    "n_zones = 1 # up to 10 zones\n",
    "# Order of bands : B, G, R, nir, nir_vegetation, swir, ndvi, ndwi, ndmi\n",
    "# Keep all bands by default and reduce if training takes too long (keep a minima the first 4 + ndvi and ndmi)\n",
    "bands_to_keep = list(range(9))\n",
    "\n",
    "# MODEL\n",
    "num_channels = len(bands_to_keep)\n",
    "num_classes = 1 # Mangrove class\n",
    "encoder_name = \"resnet50\"\n",
    "encoder_weights = None\n",
    "activation = 'sigmoid' # Mangrove vs Non-Mangrove\n",
    "use_augmentation = True\n",
    "name_model = \"UNet-Resnet50\"\n",
    "model_save_path_epochs = f'model/{name_model}/epochs/'\n",
    "os.makedirs(model_save_path_epochs, exist_ok=True)\n",
    "model_save_path_metrics = f'model/{name_model}/metrics/'\n",
    "os.makedirs(model_save_path_metrics, exist_ok=True)\n",
    "\n",
    "save_interval = 1\n",
    "\n",
    "# TRAINING\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# COMPUTATION & PRINTS (PYTORCH LIGHTNING)\n",
    "accelerator = 'gpu'\n",
    "strategy =  'auto'#'ddp' if multiple GPUs otherwise leave emtpy if single GPU training\n",
    "num_nodes = 1\n",
    "gpus_per_node = 1\n",
    "num_workers = 1\n",
    "enable_progress_bar = True\n",
    "progress_rate = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9Y_V18idsnp"
   },
   "source": [
    "# Display random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xU2mbQm0OpNh"
   },
   "outputs": [],
   "source": [
    "import image_utils  # Import the image_utils module\n",
    "\n",
    "# Now you can use functions from image_utils\n",
    "path_to_2020_sentinel_images_folder = os.path.join(dataset_dir, \"satellite-images\")\n",
    "path_to_2020_masks_folder = os.path.join(dataset_dir, \"masks\")\n",
    "\n",
    "images = image_utils.get_all_file_paths(path_to_2020_sentinel_images_folder)\n",
    "masks = image_utils.get_all_file_paths(path_to_2020_masks_folder)\n",
    "\n",
    "image_utils.display_samples(images, masks, nb_samples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "midp2J3xctdK"
   },
   "source": [
    "# Mangrove Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nhYWFXKLoVlb"
   },
   "outputs": [],
   "source": [
    "from dataset_utils import get_train_test_paths_by_zone, MangroveSegmentationDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Get train and test paths\n",
    "full_paths_train, full_paths_test = get_train_test_paths_by_zone(dataset_dir, train_test_split, n_samples_per_zone, n_zones)\n",
    "\n",
    "# Create dataset instances\n",
    "dataset_train = MangroveSegmentationDataset(full_paths_train, bands_to_keep=bands_to_keep, use_augmentation=use_augmentation)\n",
    "dataset_test = MangroveSegmentationDataset(full_paths_test, bands_to_keep=bands_to_keep, use_augmentation=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDO2n7y6hjAH"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "'''\n",
    "Swin-UMamba Code from https://github.com/JiarunLiu/Swin-UMamba\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class PatchEmbed2D(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None, **kwargs):\n",
    "        super().__init__()\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).permute(0, 2, 3, 1)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging2D(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        SHAPE_FIX = [-1, -1]\n",
    "        if (W % 2 != 0) or (H % 2 != 0):\n",
    "            print(f\"Warning, x.shape {x.shape} is not match even ===========\", flush=True)\n",
    "            SHAPE_FIX[0] = H // 2\n",
    "            SHAPE_FIX[1] = W // 2\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "\n",
    "        if SHAPE_FIX[0] > 0:\n",
    "            x0 = x0[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x1 = x1[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x2 = x2[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x3 = x3[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, H//2, W//2, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SS2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=16,\n",
    "        d_conv=3,\n",
    "        expand=2,\n",
    "        dt_rank=\"auto\",\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        dt_init=\"random\",\n",
    "        dt_scale=1.0,\n",
    "        dt_init_floor=1e-4,\n",
    "        dropout=0.,\n",
    "        conv_bias=True,\n",
    "        bias=False,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            groups=self.d_inner,\n",
    "            bias=conv_bias,\n",
    "            kernel_size=d_conv,\n",
    "            padding=(d_conv - 1) // 2,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.x_proj = (\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "        )\n",
    "        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K=4, N, inner)\n",
    "        del self.x_proj\n",
    "\n",
    "        self.dt_projs = (\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "        )\n",
    "        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0)) # (K=4, inner, rank)\n",
    "        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0)) # (K=4, inner)\n",
    "        del self.dt_projs\n",
    "\n",
    "        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
    "        self.Ds = self.D_init(self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
    "\n",
    "        self.selective_scan = selective_scan_fn\n",
    "\n",
    "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0. else None\n",
    "\n",
    "    @staticmethod\n",
    "    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init=\"random\", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, **factory_kwargs):\n",
    "        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)\n",
    "\n",
    "        # Initialize special dt projection to preserve variance at initialization\n",
    "        dt_init_std = dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
    "        dt = torch.exp(\n",
    "            torch.rand(d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "        with torch.no_grad():\n",
    "            dt_proj.bias.copy_(inv_dt)\n",
    "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        dt_proj.bias._no_reinit = True\n",
    "\n",
    "        return dt_proj\n",
    "\n",
    "    @staticmethod\n",
    "    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):\n",
    "        # S4D real initialization\n",
    "        A = repeat(\n",
    "            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\",\n",
    "            d=d_inner,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A)  # Keep A_log in fp32\n",
    "        if copies > 1:\n",
    "            A_log = repeat(A_log, \"d n -> r d n\", r=copies)\n",
    "            if merge:\n",
    "                A_log = A_log.flatten(0, 1)\n",
    "        A_log = nn.Parameter(A_log)\n",
    "        A_log._no_weight_decay = True\n",
    "        return A_log\n",
    "\n",
    "    @staticmethod\n",
    "    def D_init(d_inner, copies=1, device=None, merge=True):\n",
    "        # D \"skip\" parameter\n",
    "        D = torch.ones(d_inner, device=device)\n",
    "        if copies > 1:\n",
    "            D = repeat(D, \"n1 -> r n1\", r=copies)\n",
    "            if merge:\n",
    "                D = D.flatten(0, 1)\n",
    "        D = nn.Parameter(D)  # Keep in fp32\n",
    "        D._no_weight_decay = True\n",
    "        return D\n",
    "\n",
    "    def forward_core(self, x: torch.Tensor):\n",
    "        B, C, H, W = x.shape\n",
    "        L = H * W\n",
    "        K = 4\n",
    "\n",
    "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n",
    "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n",
    "\n",
    "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
    "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
    "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
    "\n",
    "        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n",
    "        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n",
    "        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
    "        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
    "        Ds = self.Ds.float().view(-1) # (k * d)\n",
    "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
    "        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n",
    "\n",
    "        out_y = self.selective_scan(\n",
    "            xs, dts,\n",
    "            As, Bs, Cs, Ds, z=None,\n",
    "            delta_bias=dt_projs_bias,\n",
    "            delta_softplus=True,\n",
    "            return_last_state=False,\n",
    "        ).view(B, K, -1, L)\n",
    "        assert out_y.dtype == torch.float\n",
    "\n",
    "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
    "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "\n",
    "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        xz = self.in_proj(x)\n",
    "        x, z = xz.chunk(2, dim=-1) # (b, h, w, d)\n",
    "\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = self.act(self.conv2d(x)) # (b, d, h, w)\n",
    "        y1, y2, y3, y4 = self.forward_core(x)\n",
    "        assert y1.dtype == torch.float32\n",
    "        y = y1 + y2 + y3 + y4\n",
    "        y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)\n",
    "        y = self.out_norm(y)\n",
    "        y = y * F.silu(z)\n",
    "        out = self.out_proj(y)\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VSSBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int = 0,\n",
    "        drop_path: float = 0,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        attn_drop_rate: float = 0,\n",
    "        d_state: int = 16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = SS2D(d_model=hidden_dim, dropout=attn_drop_rate, d_state=d_state, **kwargs)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        x = input + self.drop_path(self.self_attention(self.ln_1(input)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VSSLayer(nn.Module):\n",
    "    \"\"\" A basic layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        attn_drop=0.,\n",
    "        drop_path=0.,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        use_checkpoint=False,\n",
    "        d_state=16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VSSBlock(\n",
    "                hidden_dim=dim,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer,\n",
    "                attn_drop_rate=attn_drop,\n",
    "                d_state=d_state,\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "\n",
    "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
    "            def _init_weights(module: nn.Module):\n",
    "                for name, p in module.named_parameters():\n",
    "                    if name in [\"out_proj.weight\"]:\n",
    "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
    "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
    "            self.apply(_init_weights)\n",
    "\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VSSMEncoder(nn.Module):\n",
    "    def __init__(self, patch_size=4, in_chans=3, depths=[2, 2, 9, 2],\n",
    "                 dims=[96, 192, 384, 768], d_state=16, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2,\n",
    "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(depths)\n",
    "        if isinstance(dims, int):\n",
    "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.num_layers)]\n",
    "        self.embed_dim = dims[0]\n",
    "        self.num_features = dims[-1]\n",
    "        self.dims = dims\n",
    "\n",
    "        self.patch_embed = PatchEmbed2D(patch_size=patch_size, in_chans=in_chans, embed_dim=self.embed_dim,\n",
    "            norm_layer=norm_layer if patch_norm else None)\n",
    "\n",
    "        # WASTED absolute position embedding ======================\n",
    "        self.ape = False\n",
    "        if self.ape:\n",
    "            self.patches_resolution = self.patch_embed.patches_resolution\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, *self.patches_resolution, self.embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.downsamples = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = VSSLayer(\n",
    "                dim=dims[i_layer],\n",
    "                depth=depths[i_layer],\n",
    "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "            if i_layer < self.num_layers - 1:\n",
    "                self.downsamples.append(PatchMerging2D(dim=dims[i_layer], norm_layer=norm_layer))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m: nn.Module):\n",
    "        \"\"\"\n",
    "        out_proj.weight which is previously initilized in VSSBlock, would be cleared in nn.Linear\n",
    "        no fc.weight found in the any of the model parameters\n",
    "        no nn.Embedding found in the any of the model parameters\n",
    "        so the thing is, VSSBlock initialization is useless\n",
    "\n",
    "        Conv2D is not intialized !!!\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ret = []\n",
    "        x_ret.append(x)\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for s, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            x_ret.append(x.permute(0, 3, 1, 2))\n",
    "            if s < len(self.downsamples):\n",
    "                x = self.downsamples[s](x)\n",
    "\n",
    "        return x_ret\n",
    "\n",
    "\n",
    "class SwinUMamba(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans=1,\n",
    "        out_chans=13,\n",
    "        feat_size=[48, 96, 192, 384, 768],\n",
    "        drop_path_rate=0,\n",
    "        layer_scale_init_value=1e-6,\n",
    "        hidden_size: int = 768,\n",
    "        norm_name = \"instance\",\n",
    "        res_block: bool = True,\n",
    "        spatial_dims=2,\n",
    "        deep_supervision: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.feat_size = feat_size\n",
    "        self.layer_scale_init_value = layer_scale_init_value\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "              nn.Conv2d(in_chans, feat_size[0], kernel_size=7, stride=2, padding=3),\n",
    "              nn.InstanceNorm2d(feat_size[0], eps=1e-5, affine=True),\n",
    "        )\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.vssm_encoder = VSSMEncoder(patch_size=2, in_chans=feat_size[0], dims =feat_size[1:] )\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.in_chans,\n",
    "            out_channels=self.feat_size[0],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder2 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.feat_size[0],\n",
    "            out_channels=self.feat_size[1],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder3 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.feat_size[1],\n",
    "            out_channels=self.feat_size[2],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder4 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.feat_size[2],\n",
    "            out_channels=self.feat_size[3],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "\n",
    "        self.encoder5 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.feat_size[3],\n",
    "            out_channels=self.feat_size[4],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "\n",
    "        self.decoder6 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.hidden_size,\n",
    "            out_channels=self.feat_size[4],\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.hidden_size,\n",
    "            out_channels=self.feat_size[3],\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.feat_size[3],\n",
    "            out_channels=self.feat_size[2],\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.feat_size[2],\n",
    "            out_channels=self.feat_size[1],\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.feat_size[1],\n",
    "            out_channels=self.feat_size[0],\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=self.feat_size[0],\n",
    "            out_channels=self.feat_size[0],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "\n",
    "        # deep supervision support\n",
    "        self.deep_supervision = deep_supervision\n",
    "        self.out_layers = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.out_layers.append(UnetOutBlock(\n",
    "                spatial_dims=spatial_dims,\n",
    "                in_channels=self.feat_size[i],\n",
    "                out_channels=self.out_chans\n",
    "            ))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x1 = self.stem(x_in)\n",
    "        vss_outs = self.vssm_encoder(x1)\n",
    "        enc1 = self.encoder1(x_in)\n",
    "        enc2 = self.encoder2(vss_outs[0])\n",
    "        enc3 = self.encoder3(vss_outs[1])\n",
    "        enc4 = self.encoder4(vss_outs[2])\n",
    "        enc5 = self.encoder5(vss_outs[3])\n",
    "        enc_hidden = vss_outs[4]\n",
    "        dec4 = self.decoder6(enc_hidden, enc5)\n",
    "        dec3 = self.decoder5(dec4, enc4)\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        dec0 = self.decoder2(dec1, enc1)\n",
    "        dec_out = self.decoder1(dec0)\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            feat_out = [dec_out, dec1, dec2, dec3]\n",
    "            out = []\n",
    "            for i in range(4):\n",
    "                pred = self.out_layers[i](feat_out[i])\n",
    "                out.append(pred)\n",
    "        else:\n",
    "            out = self.out_layers[0](dec_out)\n",
    "\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def freeze_encoder(self):\n",
    "        for name, param in self.vssm_encoder.named_parameters():\n",
    "            if \"patch_embed\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def unfreeze_encoder(self):\n",
    "        for param in self.vssm_encoder.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVNEaPvGhjoB"
   },
   "outputs": [],
   "source": [
    "# To use SwinUMamba\n",
    "model = SwinUMamba(\n",
    "        in_chans=9,\n",
    "        out_chans=1,\n",
    "        feat_size=[35, 70, 140, 280, 560],\n",
    "        deep_supervision=False,\n",
    "        hidden_size=560,\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BS6omJOU9vUD"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    # Number of parameters in millions\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb_OTt7AmzWX"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znk-qNf7m0UJ"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=7, factor=0.5, verbose=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0cgzdbaBen8"
   },
   "outputs": [],
   "source": [
    "from train_model import train_final_model\n",
    "\n",
    "# Call the training function\n",
    "mean_loss_train, mean_f1_train, mean_iou_train, mean_accuracy_train, mean_loss_test, mean_f1_test, mean_iou_test, mean_accuracy_test, elapsed_time, best_model_filename, best_mean_iou_test = train_final_model(\n",
    "    model,\n",
    "    dataloader_train,\n",
    "    dataloader_test,\n",
    "    num_epochs,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    device,\n",
    "    model_save_path_epochs,\n",
    "    save_interval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upxzQKntCYjh"
   },
   "outputs": [],
   "source": [
    "print(f\"Elapsed time of {elapsed_time} seconds for {num_epochs} epochs => best test IOU = {best_mean_iou_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzqQ6fCye6vv"
   },
   "outputs": [],
   "source": [
    "from train_model import save_metrics_to_file\n",
    "\n",
    "lists = {\n",
    "    \"mean_loss_train\": mean_loss_train,\n",
    "    \"mean_f1_train\": mean_f1_train,\n",
    "    \"mean_iou_train\": mean_iou_train,\n",
    "    \"mean_accuracy_train\": mean_accuracy_train,\n",
    "    \"mean_loss_test\": mean_loss_test,\n",
    "    \"mean_f1_test\": mean_f1_test,\n",
    "    \"mean_iou_test\": mean_iou_test,\n",
    "    \"mean_accuracy_test\": mean_accuracy_test,\n",
    "    \"elapsed_time\": elapsed_time,\n",
    "    \"best_model_filename\": best_model_filename,\n",
    "    \"best_mean_iou_test\": best_mean_iou_test\n",
    "}\n",
    "\n",
    "save_metrics_to_file(lists, model_save_path_metrics+\"metrics.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiipxGy1mGpC"
   },
   "source": [
    "# Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDhyCzQsgjSM"
   },
   "outputs": [],
   "source": [
    "import plot_training_results\n",
    "\n",
    "# Plot training and validation loss\n",
    "plot_training_results.plot_train_val_loss(mean_loss_train, mean_loss_test)\n",
    "\n",
    "# Plot training and validation IOU score\n",
    "plot_training_results.plot_train_val_iou(mean_iou_train, mean_iou_test)\n",
    "\n",
    "# Plot training and validation F1 score\n",
    "plot_training_results.plot_train_val_f1(mean_f1_train, mean_f1_test)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plot_training_results.plot_train_val_acc(mean_accuracy_train, mean_accuracy_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVD2yeq4nB9z"
   },
   "source": [
    "# Plot segmentation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV_bfExkpbqU"
   },
   "source": [
    "## Show inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2nmoVKIjj6o",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plot_segmentation_results import plot_augmented_images\n",
    "\n",
    "# Save images and mask\n",
    "save_bands = f'model/{name_model}/plots_bands'\n",
    "os.makedirs(save_bands, exist_ok=True)\n",
    "\n",
    "# Call the function to plot augmented images\n",
    "batch = next(iter(dataloader_test))\n",
    "for idx, (test_image, true_mask) in enumerate(zip(batch[0], batch[1])):\n",
    "    test_image_np = test_image.permute(1, 2, 0).cpu().detach().numpy()\n",
    "    true_mask_np = true_mask.cpu().detach().numpy().squeeze().astype('int')\n",
    "    plot_augmented_images(test_image_np, true_mask_np, idx, save_bands)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VqcSVJCpdkH"
   },
   "source": [
    "## Show predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJrrgjYImCKk",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plot_segmentation_results import plot_segmentation_results\n",
    "\n",
    "# SHOW PREDICTIONS ON FULL BATCH\n",
    "save_comparisons = f'model/{name_model}/plots_comparison'\n",
    "os.makedirs(save_comparisons, exist_ok=True)\n",
    "\n",
    "batch = next(iter(dataloader_test))\n",
    "best_model = model\n",
    "plot_segmentation_results(batch, device, model_save_path_epochs, best_model, best_model_filename, save_comparisons)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "collapsed_sections": [
    "G9Y_V18idsnp",
    "midp2J3xctdK",
    "DDO2n7y6hjAH",
    "lb_OTt7AmzWX",
    "IiipxGy1mGpC",
    "wVD2yeq4nB9z"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
