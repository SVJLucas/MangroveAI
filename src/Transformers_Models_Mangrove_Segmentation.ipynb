{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bFpoPl8MPwBo"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT5DNsXVQgLl",
    "outputId": "bd4c760a-e2dc-4f5c-cf10-362a5744f3a7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpg3fuxhcQBm"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEjSQEdcJwLJ"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install rasterio torch pillow scikit-image pytorch-lightning segmentation-models-pytorch albumentations tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1aD5xV-5cKMY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, jaccard_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "from skimage.io import imread, imshow\n",
    "\n",
    "from skimage.io import imread\n",
    "import albumentations as A\n",
    "from typing import List\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXRXQqeH7COi"
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_all_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3NKqrA3Ig3y2"
   },
   "outputs": [],
   "source": [
    "# DATASET\n",
    "dataset_dir = \"dataset/\"\n",
    "# ADJUST THESE 2 VALUES DEPENDING ON THE TRAINING TIME\n",
    "train_test_split = 0.5\n",
    "n_samples_per_zone = 20 # 50 recommended\n",
    "n_zones = 1 # up to 10 zones\n",
    "# Order of bands : B, G, R, nir, nir_vegetation, swir, ndvi, ndwi, ndmi\n",
    "# Keep all bands by default and reduce if training takes too long (keep a minima the first 4 + ndvi and ndmi)\n",
    "bands_to_keep = list(range(9))\n",
    "\n",
    "# MODEL\n",
    "num_channels = len(bands_to_keep)\n",
    "num_classes = 1 # Mangrove class\n",
    "encoder_name = \"resnet50\"\n",
    "encoder_weights = None\n",
    "activation = 'sigmoid' # Mangrove vs Non-Mangrove\n",
    "use_augmentation = True\n",
    "name_model = \"Segformer\"\n",
    "model_save_path_epochs = f'model/{name_model}/epochs/'\n",
    "os.makedirs(model_save_path_epochs, exist_ok=True)\n",
    "model_save_path_metrics = f'model/{name_model}/metrics/'\n",
    "os.makedirs(model_save_path_metrics, exist_ok=True)\n",
    "\n",
    "save_interval = 1\n",
    "\n",
    "# TRAINING\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# COMPUTATION & PRINTS (PYTORCH LIGHTNING)\n",
    "accelerator = 'gpu'\n",
    "strategy =  'auto'#'ddp' if multiple GPUs otherwise leave emtpy if single GPU training\n",
    "num_nodes = 1\n",
    "gpus_per_node = 1\n",
    "num_workers = 1\n",
    "enable_progress_bar = True\n",
    "progress_rate = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9Y_V18idsnp"
   },
   "source": [
    "# Display random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xU2mbQm0OpNh"
   },
   "outputs": [],
   "source": [
    "import image_utils  # Import the image_utils module\n",
    "\n",
    "# Now you can use functions from image_utils\n",
    "path_to_2020_sentinel_images_folder = os.path.join(dataset_dir, \"satellite-images\")\n",
    "path_to_2020_masks_folder = os.path.join(dataset_dir, \"masks\")\n",
    "\n",
    "images = image_utils.get_all_file_paths(path_to_2020_sentinel_images_folder)\n",
    "masks = image_utils.get_all_file_paths(path_to_2020_masks_folder)\n",
    "\n",
    "image_utils.display_samples(images, masks, nb_samples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "midp2J3xctdK"
   },
   "source": [
    "# Mangrove Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nhYWFXKLoVlb"
   },
   "outputs": [],
   "source": [
    "from dataset_utils import get_train_test_paths_by_zone, MangroveSegmentationDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Get train and test paths\n",
    "full_paths_train, full_paths_test = get_train_test_paths_by_zone(dataset_dir, train_test_split, n_samples_per_zone, n_zones)\n",
    "\n",
    "# Create dataset instances\n",
    "dataset_train = MangroveSegmentationDataset(full_paths_train, bands_to_keep=bands_to_keep, use_augmentation=use_augmentation)\n",
    "dataset_test = MangroveSegmentationDataset(full_paths_test, bands_to_keep=bands_to_keep, use_augmentation=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDO2n7y6hjAH"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import SegformerModel, SegformerConfig\n",
    "\n",
    "class SegFormer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SegFormer,self).__init__()\n",
    "\n",
    "\n",
    "    configuration = SegformerConfig(\n",
    "        num_channels = 9,\n",
    "        num_labels = 1,\n",
    "        num_encoder_blocks = 4,\n",
    "        depths = [3, 3, 3, 3],\n",
    "        sr_ratios = [8, 4, 2, 1],\n",
    "        hidden_sizes = [256, 256, 256, 256],\n",
    "        patch_sizes = [7, 3, 3, 3],\n",
    "        strides = [4, 2, 2, 2],\n",
    "        num_attention_heads = [8, 8, 8, 8],\n",
    "        mlp_ratios = [8, 8, 8, 8],\n",
    "        hidden_act = 'gelu',\n",
    "        hidden_dropout_prob = 0.0,\n",
    "        attention_probs_dropout_prob = 0.0,\n",
    "        classifier_dropout_prob = 0.1,\n",
    "        initializer_range = 0.02,\n",
    "        drop_path_rate = 0.1,\n",
    "        layer_norm_eps = 1e-06,\n",
    "        decoder_hidden_size = 128)\n",
    "\n",
    "    self.transformer = SegformerForSemanticSegmentation(configuration)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self.relu = nn.ReLU()\n",
    "    self.upsample = nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1)\n",
    "\n",
    "  def forward(self, images, output_size = (128,128)):\n",
    "    images = self.transformer(images).logits\n",
    "    images = self.upsample(images, output_size = (output_size[0]//2,output_size[1]//2))\n",
    "    images = self.relu(images)\n",
    "    images = self.upsample(images, output_size = output_size)\n",
    "    images = self.sigmoid(images)\n",
    "    return images\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import BeitForSemanticSegmentation, BeitConfig\n",
    "\n",
    "class BEiT(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BEiT,self).__init__()\n",
    "\n",
    "\n",
    "    configuration = BeitConfig(\n",
    "        num_labels = 1,\n",
    "        vocab_size=8192,\n",
    "        hidden_size=264,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3172,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.0,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        image_size=128,\n",
    "        patch_size=16,\n",
    "        num_channels=9,\n",
    "        use_mask_token=False,\n",
    "        use_absolute_position_embeddings=False,\n",
    "        use_relative_position_bias=False,\n",
    "        use_shared_relative_position_bias=False,\n",
    "        layer_scale_init_value=0.1,\n",
    "        drop_path_rate=0.1,\n",
    "        use_mean_pooling=True,\n",
    "        pool_scales=[1, 2, 3, 6],\n",
    "        use_auxiliary_head=True,\n",
    "        auxiliary_loss_weight=0.4,\n",
    "        auxiliary_channels=256,\n",
    "        auxiliary_num_convs=1,\n",
    "        auxiliary_concat_input=False,\n",
    "        semantic_loss_ignore_index=255,\n",
    "        out_features=None,\n",
    "        out_indices=[3, 5, 7, 11],\n",
    "        add_fpn=False,\n",
    "        reshape_hidden_states=True,\n",
    "    )\n",
    "\n",
    "    self.transformer = BeitForSemanticSegmentation(configuration)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self.relu = nn.ReLU()\n",
    "    self.upsample = nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1)\n",
    "\n",
    "  def forward(self, images, output_size = (128,128)):\n",
    "    images = self.transformer(images).logits\n",
    "    images = self.upsample(images, output_size = (output_size[0]//2,output_size[1]//2))\n",
    "    images = self.relu(images)\n",
    "    images = self.upsample(images, output_size = output_size)\n",
    "    images = self.sigmoid(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVNEaPvGhjoB"
   },
   "outputs": [],
   "source": [
    "# To use Segformer\n",
    "model = SegFormer()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# To use BEiT\n",
    "# model = BEiT()"
   ],
   "metadata": {
    "id": "NWHiVTfQ-eqM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BS6omJOU9vUD"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    # Number of parameters in millions\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb_OTt7AmzWX"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znk-qNf7m0UJ"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=7, factor=0.5, verbose=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0cgzdbaBen8"
   },
   "outputs": [],
   "source": [
    "from train_model import train_final_model\n",
    "\n",
    "# Call the training function\n",
    "mean_loss_train, mean_f1_train, mean_iou_train, mean_accuracy_train, mean_loss_test, mean_f1_test, mean_iou_test, mean_accuracy_test, elapsed_time, best_model_filename, best_mean_iou_test = train_final_model(\n",
    "    model,\n",
    "    dataloader_train,\n",
    "    dataloader_test,\n",
    "    num_epochs,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    device,\n",
    "    model_save_path_epochs,\n",
    "    save_interval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upxzQKntCYjh"
   },
   "outputs": [],
   "source": [
    "print(f\"Elapsed time of {elapsed_time} seconds for {num_epochs} epochs => best test IOU = {best_mean_iou_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzqQ6fCye6vv"
   },
   "outputs": [],
   "source": [
    "from train_model import save_metrics_to_file\n",
    "\n",
    "lists = {\n",
    "    \"mean_loss_train\": mean_loss_train,\n",
    "    \"mean_f1_train\": mean_f1_train,\n",
    "    \"mean_iou_train\": mean_iou_train,\n",
    "    \"mean_accuracy_train\": mean_accuracy_train,\n",
    "    \"mean_loss_test\": mean_loss_test,\n",
    "    \"mean_f1_test\": mean_f1_test,\n",
    "    \"mean_iou_test\": mean_iou_test,\n",
    "    \"mean_accuracy_test\": mean_accuracy_test,\n",
    "    \"elapsed_time\": elapsed_time,\n",
    "    \"best_model_filename\": best_model_filename,\n",
    "    \"best_mean_iou_test\": best_mean_iou_test\n",
    "}\n",
    "\n",
    "save_metrics_to_file(lists, model_save_path_metrics+\"metrics.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiipxGy1mGpC"
   },
   "source": [
    "# Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDhyCzQsgjSM"
   },
   "outputs": [],
   "source": [
    "import plot_training_results\n",
    "\n",
    "# Plot training and validation loss\n",
    "plot_training_results.plot_train_val_loss(mean_loss_train, mean_loss_test)\n",
    "\n",
    "# Plot training and validation IOU score\n",
    "plot_training_results.plot_train_val_iou(mean_iou_train, mean_iou_test)\n",
    "\n",
    "# Plot training and validation F1 score\n",
    "plot_training_results.plot_train_val_f1(mean_f1_train, mean_f1_test)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plot_training_results.plot_train_val_acc(mean_accuracy_train, mean_accuracy_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVD2yeq4nB9z"
   },
   "source": [
    "# Plot segmentation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV_bfExkpbqU"
   },
   "source": [
    "## Show inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2nmoVKIjj6o",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plot_segmentation_results import plot_augmented_images\n",
    "\n",
    "# Save images and mask\n",
    "save_bands = f'model/{name_model}/plots_bands'\n",
    "os.makedirs(save_bands, exist_ok=True)\n",
    "\n",
    "# Call the function to plot augmented images\n",
    "batch = next(iter(dataloader_test))\n",
    "for idx, (test_image, true_mask) in enumerate(zip(batch[0], batch[1])):\n",
    "    test_image_np = test_image.permute(1, 2, 0).cpu().detach().numpy()\n",
    "    true_mask_np = true_mask.cpu().detach().numpy().squeeze().astype('int')\n",
    "    plot_augmented_images(test_image_np, true_mask_np, idx, save_bands)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VqcSVJCpdkH"
   },
   "source": [
    "## Show predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJrrgjYImCKk",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plot_segmentation_results import plot_segmentation_results\n",
    "\n",
    "# SHOW PREDICTIONS ON FULL BATCH\n",
    "save_comparisons = f'model/{name_model}/plots_comparison'\n",
    "os.makedirs(save_comparisons, exist_ok=True)\n",
    "\n",
    "batch = next(iter(dataloader_test))\n",
    "best_model = model\n",
    "plot_segmentation_results(batch, device, model_save_path_epochs, best_model, best_model_filename, save_comparisons)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "collapsed_sections": [
    "G9Y_V18idsnp",
    "midp2J3xctdK",
    "DDO2n7y6hjAH",
    "lb_OTt7AmzWX",
    "IiipxGy1mGpC",
    "wVD2yeq4nB9z"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
